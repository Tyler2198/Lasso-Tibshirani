{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef28e739",
   "metadata": {},
   "source": [
    "# Regression Shrinkage and Selection via the Lasso - Complete Code and Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e867b661",
   "metadata": {},
   "source": [
    "## By Robert Tibshirani - University of Toronto (1995)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b2c43a",
   "metadata": {},
   "source": [
    "## Analysis and Implementation by Denis L. Cascino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c187a116",
   "metadata": {},
   "source": [
    "### 1. Statement of Purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcc2b08",
   "metadata": {},
   "source": [
    "### 2. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fea4df",
   "metadata": {},
   "source": [
    "To really understand the main reasons why Lasso has been introduced in the Statistical and Machine Learning world, it would be beneficial to briefly think about the challenges that OLS poses and the solutions that were already available, their pros and their cons.\n",
    "\n",
    "We can start out by considering an available dataset of labeled data points, $\\{(\\textbf{x}_i,y_i)\\}_{i=1}^{N}$, where $\\textbf{x}_i=(x_1^i \\ x_2^i \\ ...\\ x_p^i)^T $.\n",
    "\n",
    "At its essence, we are trying to fit a linear model to our available data. The linear model will take the following functional form:\n",
    "\n",
    "\\begin{equation}\n",
    "y_i = w_0 \\ + \\ \\textbf{w}^T\\textbf{x}_i \\ + \\  \\epsilon_i\n",
    "\\end{equation}\n",
    "\n",
    "$\\forall i=1,2,..., N.$\n",
    "\n",
    "Here, $w_0$ and $\\textbf{w}=(w_1 \\ w_2 \\ ... \\ w_p)^T$ are, respectively, the bias and the vector of coefficients that will have to be learned and optimised.\n",
    "\n",
    "The Ordinary Least Squares (OLS) estimates of the coefficients of the linear regression are obtained by minimising the sum of residual squared errors, formally:\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{w_0, \\textbf{w}} \\ \\ \\  J(w_0, \\textbf{w}) = \\frac{1}{2}\\sum_{i=1}^N(y_i \\ - \\ w_0  \\ - \\ \\textbf{w}^T\\textbf{x}_i)^2\n",
    "\\end{equation}\n",
    "\n",
    "There are two main issues with this procedure:\n",
    "\n",
    "- __Prediction Accuracy__: OLS outputs coefficient estimates that find a minimum of the loss function in the training set. This may lead to overfitting of the training data and over-capturing random noise detrimental to the model’s generalisation power. As such, the model will have low bias but high variance to data changes. This will negatively impact the prediction accuracy.\n",
    "\n",
    "- __Interpretability__: OLS may retain a large number of predictors, even when their forecasting power is minimal. In contrast, we would want a model that selects a small subsets of predictors, mainly those that explain the bulk of the variability of the target variable.\n",
    "\n",
    "Until Lasso, the two main ways to face these issues were encompassed by two specific models with their corresponding benefits and costs:\n",
    "\n",
    "1) __Subset Selection__ : this is a discrete process where some coefficients are completely excluded, while others are retained by means of an optimisation problem. As such, the benefits in terms of model interpretability are enormous, however the model variance becomes a relevant issue. The discrete nature of the exclusion process makes the choice of relevant coefficients very sensitive to small changes in the data available.\n",
    "\n",
    "2) __Ridge Regression__ : this allows for continuous shrinkage of coefficients, thus highly mitigating variance issues relative to subset selection. However, coefficients are not really excluded and are rarely shrinked to zero. This becomes detrimental to model interpretability.\n",
    "\n",
    "This is the reason why Lasso is proposed. The new operator combines the properties of both subset selection and Ridge Regression, allowing for interpretability and variance mitigation by means of continuous shrinkage of coefficients and zero-setting of some of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736db014",
   "metadata": {},
   "source": [
    "### 3. The Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a25353a",
   "metadata": {},
   "source": [
    "### 3.1 Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9899a2a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eabb9a50",
   "metadata": {},
   "source": [
    "## Shrinkage Comparison\n",
    "\n",
    "The following provides the plots for coefficient shrinkagings from the different regressions, namely subset selection, ridge, lasso and garotte. The plot comes with a slider that lets the user adjust gamma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d81df64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b187a85d4b934e3084bae85a7c1edaa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=2.0, description='Gamma', max=5.0), Output()), _dom_classes=('widget-i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_four_subplots(beta_value)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact, FloatSlider\n",
    "\n",
    "# Set a nice \"research-y\" style/theme for Seaborn\n",
    "sns.set_theme(\n",
    "    style='white',         # pure white background, no grid lines\n",
    "    context='talk',\n",
    "    palette='deep'         # Seaborn’s default “deep” palette\n",
    ")\n",
    "\n",
    "def plot_four_subplots(beta_value):\n",
    "    x = np.linspace(1e-6, 5, 200)      # range for plotting\n",
    "    identity_line = x              # dotted line y = x\n",
    "\n",
    "    # -- Example relationships to mimic the shapes in your subplots --\n",
    "    y_a = np.where(x < beta_value, 0, x)\n",
    "    y_b = (1 / (1 + beta_value)) * x\n",
    "    y_c = np.maximum(abs(x) - beta_value, 0)\n",
    "    y_d = np.maximum((1 - (beta_value / (x**2)))*x, 0)\n",
    "\n",
    "    # Create the figure and axes\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 6))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    # Subplot (a)\n",
    "    sns.lineplot(ax=axes[0], x=x, y=identity_line, \n",
    "                 color='gray', linestyle=':', label='45° line')\n",
    "    sns.lineplot(ax=axes[0], x=x, y=y_a, \n",
    "                 color='b')\n",
    "    axes[0].set_title('(a)')\n",
    "    axes[0].set_xlabel('Subset Selection Beta')\n",
    "    axes[0].set_ylabel('OLS Coefficient')\n",
    "    axes[0].set_ylim([-1, 6])\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Subplot (b)\n",
    "    sns.lineplot(ax=axes[1], x=x, y=identity_line, \n",
    "                 color='gray', linestyle=':', label='45° line')\n",
    "    sns.lineplot(ax=axes[1], x=x, y=y_b, \n",
    "                 color='r')\n",
    "    axes[1].set_title('(b)')\n",
    "    axes[1].set_xlabel('Ridge Beta')\n",
    "    axes[1].set_ylim([-1, 6])\n",
    "    axes[1].legend()\n",
    "\n",
    "    # Subplot (c)\n",
    "    sns.lineplot(ax=axes[2], x=x, y=identity_line, \n",
    "                 color='gray', linestyle=':', label='45° line')\n",
    "    sns.lineplot(ax=axes[2], x=x, y=y_c, \n",
    "                 color='g')\n",
    "    axes[2].set_title('(c)')\n",
    "    axes[2].set_xlabel('Lasso Beta')\n",
    "    axes[2].set_ylabel('OLS Coefficient')\n",
    "    axes[2].set_ylim([-1, 6])\n",
    "    axes[2].legend()\n",
    "\n",
    "    # Subplot (d)\n",
    "    sns.lineplot(ax=axes[3], x=x, y=identity_line, \n",
    "                 color='gray', linestyle=':', label='45° line')\n",
    "    sns.lineplot(ax=axes[3], x=x, y=y_d, \n",
    "                 color='m')\n",
    "    axes[3].set_title('(d)')\n",
    "    axes[3].set_xlabel('Garotte Beta')\n",
    "    axes[3].set_ylim([-1, 6])\n",
    "    axes[3].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create the interactive slider\n",
    "interact(\n",
    "    plot_four_subplots,\n",
    "    beta_value=FloatSlider(\n",
    "        value=2.0,  # initial\n",
    "        min=0,\n",
    "        max=5,\n",
    "        continuous_update=True,\n",
    "        step=0.1,\n",
    "        description='Gamma'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acb4204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
